{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(b' ', b't'), (b' ', b'a'), (b'h', b'e'), (b'i', b'n'), (b'r', b'e'), (b' t', b'he'), (b'o', b'n'), (b'e', b'r'), (b' ', b's'), (b' ', b'w')]\n",
      "[(1000, b'way'), (1001, b' government'), (1002, b'gg'), (1003, b' Re'), (1004, b'led'), (1005, b'ner'), (1006, b'hip'), (1007, b'atch'), (1008, b'hes'), (1009, b'ames'), (1010, b'any'), (1011, b' ter'), (1012, b' somet'), (1013, b'cent'), (1014, b'velop'), (1015, b'ted'), (1016, b'We'), (1017, b' ref'), (1018, b' trans'), (1019, b'ten'), (1020, b' ins'), (1021, b' run'), (1022, b' ser'), (1023, b' found'), (1024, b' three'), (1025, b'ives'), (1026, b'ave'), (1027, b' mem'), (1028, b' around'), (1029, b' here'), (1030, b'ather'), (1031, b' med'), (1032, b' real'), (1033, b' used'), (1034, b'ouse'), (1035, b' 10'), (1036, b'ved'), (1037, b' exp'), (1038, b'az'), (1039, b' Ar'), (1040, b'cept'), (1041, b'oot'), (1042, b' system'), (1043, b' You'), (1044, b'ci'), (1045, b' really'), (1046, b' mod'), (1047, b'ving'), (1048, b'con'), (1049, b'cer'), (1050, b' group'), (1051, b' support'), (1052, b' 7'), (1053, b' cour'), (1054, b'ets'), (1055, b'ale'), (1056, b'ins'), (1057, b' ret'), (1058, b' hand'), (1059, b' public'), (1060, b' spec'), (1061, b'pr'), (1062, b' Le'), (1063, b'ying'), (1064, b' great'), (1065, b' car'), (1066, b'ense'), (1067, b' ext'), (1068, b'.['), (1069, b'ton'), (1070, b'irect'), (1071, b' cur'), (1072, b' fam'), (1073, b' Com'), (1074, b'alth'), (1075, b' happ'), (1076, b' fact'), (1077, b'ph'), (1078, b' exper'), (1079, b' They'), (1080, b' Trump'), (1081, b' fr'), (1082, b' wom'), (1083, b' Is'), (1084, b' cap'), (1085, b' both'), (1086, b' day'), (1087, b' find'), (1088, b' Al'), (1089, b'ular'), (1090, b' read'), (1091, b'ines'), (1092, b'oh'), (1093, b'ices'), (1094, b' det'), (1095, b' lit'), (1096, b' sl'), (1097, b' rep'), (1098, b'uth'), (1099, b'ered')]\n",
      "b'\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82'\n",
      "ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ\n"
     ]
    }
   ],
   "source": [
    "# LONGEST VOCAB TOKEN\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"../out/tokenizers/owt_train-merges.txt\", \"rb\") as f:\n",
    "    merges = pickle.load(f)\n",
    "\n",
    "with open(\"../out/tokenizers/owt_train-vocab-32000.txt\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "print(merges[:10])\n",
    "print(list(vocab.items())[1000:1100])\n",
    "\n",
    "longest_vocab_token: bytes = sorted(list(vocab.values()), key=lambda x: len(x), reverse=True)[0]\n",
    "print(longest_vocab_token)\n",
    "print(longest_vocab_token.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = None # NOTE: set your data dir path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.012658227848101\n"
     ]
    }
   ],
   "source": [
    "# COMPRESSION RATIO\n",
    "\n",
    "from cs336_basics.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_files(\"../out/tokenizers/TinyStoriesV2-GPT4-train-vocab-10000.txt\", \"../out/tokenizers/TinyStoriesV2-GPT4-train-merges.txt\", [\"<|endoftext|>\"])\n",
    "# tokenizer = Tokenizer.from_files(\"../out/tokenizers/owt_train-vocab-32000.txt\", \"../out/tokenizers/owt_train-merges.txt\", [\"<|endoftext|>\"])\n",
    "\n",
    "data_path = data_dir + \"TinyStoriesV2-GPT4-valid.txt\"\n",
    "# data_path = data_dir + \"owt_valid.txt\"\n",
    "\n",
    "with open(data_path) as f:\n",
    "    contents = f.read()\n",
    "\n",
    "# Sample 10 docs\n",
    "docs = contents.split(\"<|endoftext|>\")[:10]\n",
    "\n",
    "ids = []\n",
    "doc_bytes = []\n",
    "\n",
    "for doc in docs:\n",
    "    # print(doc)\n",
    "    ids.extend(tokenizer.encode(doc))\n",
    "    doc_bytes.extend(bytes(doc, encoding=\"UTF-8\"))\n",
    "\n",
    "num_tokens = len(ids)\n",
    "num_bytes = len(doc_bytes)\n",
    "\n",
    "print(num_bytes / num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurrences: 4679\n"
     ]
    }
   ],
   "source": [
    "# FREQUENCY OF TOKEN IN DATASET\n",
    "\n",
    "target_bytes = b\"\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\"\n",
    "count = 0\n",
    "\n",
    "data_path = data_dir + \"owt_train.txt\"\n",
    "\n",
    "with open(data_path, \"rb\") as f:\n",
    "    buffer = b\"\"\n",
    "    chunk_size = 1024 * 1024\n",
    "    while True:\n",
    "        chunk = f.read(chunk_size)\n",
    "        if not chunk:\n",
    "            break\n",
    "        buffer += chunk\n",
    "        count += buffer.count(target_bytes)\n",
    "        buffer = buffer[-len(target_bytes) :]\n",
    "\n",
    "print(f\"Occurrences: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,461,204 tokens processed\n",
      "22,502,601 bytes processed\n",
      "3,378,131.08 bytes/s\n",
      "819,845.80 tok/s\n",
      "98.90% cache hits\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZER THROUGHPUT IN BYTES/SECOND\n",
    "\n",
    "import time\n",
    "from cs336_basics.tokenizer import Tokenizer\n",
    "import os\n",
    "\n",
    "tokenizer = Tokenizer.from_files(\"../out/tokenizers/TinyStoriesV2-GPT4-train-vocab-10000.txt\", \"../out/tokenizers/TinyStoriesV2-GPT4-train-merges.txt\", [\"<|endoftext|>\"])\n",
    "\n",
    "data_path = data_dir + \"TinyStoriesV2-GPT4-valid.txt\"\n",
    "file_size = os.path.getsize(data_path) # in bytes\n",
    "ct = 0\n",
    "with open(data_path) as f:\n",
    "    token_stream = tokenizer.encode_iterable(f)\n",
    "\n",
    "    start_time = time.time()\n",
    "    while next(token_stream, -1) != -1:\n",
    "        ct += 1\n",
    "    end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "bytes_per_second = file_size / total_time\n",
    "\n",
    "print(f\"{ct:,} tokens processed\")\n",
    "print(f\"{file_size:,} bytes processed\")\n",
    "print(f\"{bytes_per_second:,.2f} bytes/s\")\n",
    "print(f\"{ct / total_time:,.2f} tok/s\")\n",
    "print(f\"{tokenizer.cache_hits / ct:.2%} cache hits\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
